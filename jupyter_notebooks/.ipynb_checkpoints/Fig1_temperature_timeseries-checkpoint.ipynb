{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279d70e-a034-4a22-98bd-0bde70e63395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_file_info(d):\n",
    "    \n",
    "    #Mapping between levels\n",
    "    level_dict = {'L1': 'El Nino Static SST. ablco2=6',\n",
    "                  'L2': 'EC Earth SST. ablco2=6',\n",
    "                  'L3': 'El Nino Static SST. ablco2=21',\n",
    "                  'L4': 'EC Earth SST. ablco2=21'\n",
    "                  }\n",
    "\n",
    "    #Create some useful labels\n",
    "    label = d.split('/')[-1]\n",
    "    level = label.split('_')[1]\n",
    "    prec = label.split('_')[2]\n",
    "    rounding = label.split('_')[3]\n",
    "    title = level_dict[level]\n",
    "    \n",
    "    keys = ['label', 'level', 'precision','title']\n",
    "    values = [label,level,prec+'_'+rounding,title]\n",
    "    dictionary = {keys[i]: values[i] for i in range(len(keys))}\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def surface_slice(y,weights):\n",
    "    \n",
    " \n",
    "    #Get values at surface\n",
    "    y_surface =  y[:,0]   \n",
    "    ygrid = y_surface.weighted(weights).mean((\"longitude\", \"latitude\")).data #average across every surface grid point\n",
    "\n",
    "\n",
    "    return ygrid\n",
    "\n",
    "\n",
    "def extract_file_data(df,columns,weights):\n",
    "    \n",
    "    \n",
    "    data = [surface_slice(getattr(df,\n",
    "                                  columns[k]),\n",
    "                                   weights = weights) for k in range(len(columns))]\n",
    "    \n",
    "    \n",
    "    dictionary = {columns[i]: data[i] for i in range(len(columns))}\n",
    "    df_out = pd.DataFrame(dictionary)\n",
    "    \n",
    "    \n",
    "    return df_out\n",
    "\n",
    "    \n",
    "    \n",
    "def process_nc_file(fname,weights,directory,true_lat):    \n",
    "    \n",
    "        #Get data \n",
    "        df = xr.open_dataset(fname)\n",
    "        \n",
    "        \n",
    "        #Reset the latitude\n",
    "        df = df.assign_coords(latitude=(true_lat))\n",
    "        \n",
    "\n",
    "       \n",
    "        #Get the data you want     \n",
    "        columns_3d  = ['temperature']\n",
    "        data3d = extract_file_data(df,columns_3d,weights)\n",
    "\n",
    "                \n",
    "        #Get meta information\n",
    "        f_info = extract_file_info(directory)\n",
    "        index = data3d.index\n",
    "        df_info = pd.DataFrame(f_info,index=index)\n",
    "      \n",
    "        #Create pandas df and append\n",
    "        df_meta = pd.concat([data3d,df_info,],axis=1) \n",
    "        \n",
    "        return df_meta\n",
    "        \n",
    "    \n",
    "        \n",
    "def process_all_data(all_directories,weights,true_lat):\n",
    "\n",
    "    \n",
    "    #Empty arrays to hold data\n",
    "    dfs = []\n",
    "    \n",
    "\n",
    "    for d in tqdm(all_directories): #for every directory\n",
    "        df_LI = [] # df of solution level i\n",
    "        nc_files = sorted(glob.glob(d+'/model_output*.nc'))\n",
    "                \n",
    "        for n in tqdm(nc_files): #for every model.nc file\n",
    "\n",
    "            df_meta = process_nc_file(n,weights,d,true_lat)\n",
    "            df_LI.append(df_meta)\n",
    "        \n",
    "        df_LI = pd.concat(df_LI,ignore_index=True)\n",
    "        df_LI['X'] = np.arange(len(df_LI))\n",
    "        \n",
    "        #Add append to bigger array\n",
    "        dfs.append(df_LI)\n",
    "            \n",
    "    #Bring all together\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    return df #.reset_index()\n",
    "\n",
    "    \n",
    "def get_global_weights():\n",
    "    \n",
    "     #Get the latitude weights from a special location\n",
    "    r1 = '/network/group/aopp/predict/TIP016_PAXTON_RPSPEEDY/speedyone/paper/Fig1_10year/'\n",
    "    f = r1 + 'speedyoneFIG1_L2_52_RN_10year/model_output00001.nc'\n",
    "    df = xr.open_dataset(f)\n",
    "    \n",
    "    temp_file = df.temperature\n",
    "    weights = np.cos(np.deg2rad(temp_file.latitude))\n",
    "         \n",
    "    weights.name = \"weights\"\n",
    "    \n",
    "    \n",
    "    return weights, temp_file.latitude\n",
    "    \n",
    "\n",
    "def process_node(node):\n",
    "    \n",
    "    root= '/network/group/aopp/predict/TIP016_PAXTON_RPSPEEDY/speedyone/'+node\n",
    "    \n",
    "   \n",
    "    #Get the global weights\n",
    "    weights,true_lat = get_global_weights()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #iterate over every directory\n",
    "    all_dirs = glob.glob(root+'speedyone*')     \n",
    "    df = process_all_data(all_dirs,weights,true_lat)\n",
    "    \n",
    "     \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fbdef-6792-436b-83f9-fb7a091666b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
